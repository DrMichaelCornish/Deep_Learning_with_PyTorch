{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[4.,1.],\n",
    "                       [5.,3.],\n",
    "                       [2.,1.]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.,1.],\n",
    "                       [5.,3.],\n",
    "                       [2.,1.]\n",
    "                       ])\n",
    "\n",
    "points[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape does not keep the geometry of the tensory, \n",
    "# i.e row vector vs column vector extracted from the original tensor\n",
    "points[:,1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = list(range(6))\n",
    "some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[0:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3,5,5) # shape [channels (colors), rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722]) # weights for combining color channels to get grey scale brightness (not trivial, aparently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine now that we have a batch of images, so the leading dimension is the batch size\n",
    "# here we have 2 images\n",
    "batch_t = torch.randn(2,3,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3) # take the mean for the third to last dimension, i.e. the color channel\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2126, 0.7152, 0.0722])\n",
      "tensor([[0.2126],\n",
      "        [0.7152],\n",
      "        [0.0722]])\n",
      "tensor([[[0.2126]],\n",
      "\n",
      "        [[0.7152]],\n",
      "\n",
      "        [[0.0722]]])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "We want to multiply the weights by the color channel values.\n",
    "To do this, we need to unsqueeze the weights tensor, which means\n",
    "it pads the tensor with an additional trailing dimension.\n",
    "\n",
    "If we do img_t * weights the dimensions won't line up\n",
    "\n",
    "'''\n",
    "print(weights)\n",
    "print(weights.unsqueeze(-1)) # this adds an extra dimension at dimension index -1, i.e an extra final (empty) dimension \n",
    "print(weights.unsqueeze(-1).unsqueeze(-1)) # same thing by then adds another empty dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5994, -0.2495, -1.9543,  1.6787,  1.6019],\n",
      "         [-1.2424,  1.2549,  0.1898, -0.0081, -0.5678],\n",
      "         [-1.1053, -0.0828, -0.8733, -0.6413, -0.1430],\n",
      "         [-0.2015,  0.0253,  1.8150,  0.9519, -1.5756],\n",
      "         [ 0.4411, -1.5761,  2.0428, -0.3933, -0.7832]],\n",
      "\n",
      "        [[-0.7239,  0.5119, -0.1204,  0.8924, -0.4763],\n",
      "         [ 0.9363, -0.3296, -0.6300, -0.3760, -0.9687],\n",
      "         [ 0.4823, -1.4348, -0.2831,  1.1522,  1.1197],\n",
      "         [-0.5467,  0.3775, -0.7526, -0.2951,  0.4149],\n",
      "         [-0.0986,  0.4345, -0.0718,  0.4520, -0.2145]],\n",
      "\n",
      "        [[ 0.3985, -0.2885,  1.1508,  0.3642,  0.7306],\n",
      "         [-0.1079, -0.5735,  0.4911,  0.3113,  1.5754],\n",
      "         [-0.8418, -0.3111,  0.4578, -0.1271, -1.0427],\n",
      "         [-0.2386,  1.3738, -0.4204,  2.1151,  1.6561],\n",
      "         [ 0.8965,  1.4425,  0.3640,  2.0232,  0.2012]]])\n",
      "tensor([[[-0.1274, -0.0531, -0.4155,  0.3569,  0.3406],\n",
      "         [-0.2641,  0.2668,  0.0404, -0.0017, -0.1207],\n",
      "         [-0.2350, -0.0176, -0.1857, -0.1363, -0.0304],\n",
      "         [-0.0428,  0.0054,  0.3859,  0.2024, -0.3350],\n",
      "         [ 0.0938, -0.3351,  0.4343, -0.0836, -0.1665]],\n",
      "\n",
      "        [[-0.5177,  0.3661, -0.0861,  0.6382, -0.3407],\n",
      "         [ 0.6697, -0.2357, -0.4506, -0.2689, -0.6928],\n",
      "         [ 0.3450, -1.0262, -0.2025,  0.8240,  0.8008],\n",
      "         [-0.3910,  0.2700, -0.5383, -0.2111,  0.2967],\n",
      "         [-0.0705,  0.3107, -0.0513,  0.3233, -0.1534]],\n",
      "\n",
      "        [[ 0.0288, -0.0208,  0.0831,  0.0263,  0.0527],\n",
      "         [-0.0078, -0.0414,  0.0355,  0.0225,  0.1137],\n",
      "         [-0.0608, -0.0225,  0.0331, -0.0092, -0.0753],\n",
      "         [-0.0172,  0.0992, -0.0304,  0.1527,  0.1196],\n",
      "         [ 0.0647,  0.1041,  0.0263,  0.1461,  0.0145]]])\n"
     ]
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1) # pad 2 extra trailing dimensions\n",
    "img_weights = img_t * unsqueezed_weights\n",
    "print(img_t)\n",
    "print(img_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights = batch_t * unsqueezed_weights # note that broadcasting with torch will handing leading dimension (it starts alignment with trailing dimensions)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can invoke Einstien summation (einsum), which is an idea copied from numpy\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights) # indicates the 3 dimensions can be labelled as c, h, & w. Broadcasting should leave h & w.\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/f9tvwl412dq2pq6wwlstvskh0000gp/T/ipykernel_80005/3180387998.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/c10/core/TensorImpl.h:1938.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names = ['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named Tensors (like a dictionary)\n",
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names = ['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(...,'channels','rows','columns') # the ellipsis (...) ignores all but the final dimensons\n",
    "batch_named = batch_t.refine_names(...,'channels','rows','columns')\n",
    "print(f\"img named: {img_named.shape} {img_named.names}\")\n",
    "print(f\"batch named: {batch_named.shape} {batch_named.names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can have torch pad the trailing and leading dimensions to ensure one tensor matches the dimenson of the other appropriately\n",
    "weights_aligned = weights_named.align_as(img_named) # make weights_named match such that the channels columns align\n",
    "weights_aligned.shape, weights_aligned.names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels') # weighted sum as before, but now more readable\n",
    "gray_named.shape, gray_named.names # channels dimension gets summed away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5994, -0.2495, -1.9543],\n",
       "         [-1.2424,  1.2549,  0.1898],\n",
       "         [-1.1053, -0.0828, -0.8733],\n",
       "         [-0.2015,  0.0253,  1.8150],\n",
       "         [ 0.4411, -1.5761,  2.0428]],\n",
       "\n",
       "        [[-0.7239,  0.5119, -0.1204],\n",
       "         [ 0.9363, -0.3296, -0.6300],\n",
       "         [ 0.4823, -1.4348, -0.2831],\n",
       "         [-0.5467,  0.3775, -0.7526],\n",
       "         [-0.0986,  0.4345, -0.0718]],\n",
       "\n",
       "        [[ 0.3985, -0.2885,  1.1508],\n",
       "         [-0.1079, -0.5735,  0.4911],\n",
       "         [-0.8418, -0.3111,  0.4578],\n",
       "         [-0.2386,  1.3738, -0.4204],\n",
       "         [ 0.8965,  1.4425,  0.3640]]], names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mimg_named\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_named\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "img_named * weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It catches that we shouldn't be multiplying these two tensors (without proper alignment first, of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get back to unnamed tensors\n",
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgray_plain\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_named\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "gray_plain * weights_named # now the error is in the number of values per dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1310,  0.2090, -0.0302],\n",
       "        [ 0.0846, -0.0074, -0.0271],\n",
       "        [ 0.0105, -0.7626, -0.0256],\n",
       "        [-0.0959,  0.2679, -0.0132],\n",
       "        [ 0.0187,  0.0571,  0.0295]], names=(None, 'channels'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we ensure the last dimension takes the first 3 values, \n",
    "# meaning that we can now multiply the 3 valued weights_named\n",
    "# by the final column\n",
    "gray_plain[...,:3] * weights_named "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the named information is carried over. Of course\n",
    "# this would be an incorrect operation. It just illustrates\n",
    "# the point that using names would help ensure we are performing\n",
    "# the operations that we think we are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default dtype for values in a tensor is float = 32bit number\n",
    "# you can use tensors to represent indexes of other tensors\n",
    "# these are typically stored with int64 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.ones(10,2, dtype = torch.double)\n",
    "short_points = torch.tensor([[1,2],[3,4]], dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.zeros(10,2).double() # convert to double precision\n",
    "short_points = torch.ones(10,2).short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more convenient method (I don't think so)\n",
    "double_points = torch.zeros(10,2).to(torch.double)\n",
    "short_points = torch.ones(10,2).to(torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, -0.0000, 1.0237, -0.0000, -0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplying different types defaults to larger type\n",
    "points_64 = torch.randn(5, dtype=torch.double)\n",
    "points_short = points_64.short()\n",
    "points_64 * points_short # notice the output dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Tensor API\n",
    "# note it is an API because we are really calling some C functions, or applications\n",
    "# to do the heavy but efficient lifting under the hood\n",
    "a = torch.ones(3,2)\n",
    "a_t = torch.transpose(a,0,1) # transport dimension 0 (rows) to dimension 1 (columns)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDocstring:\u001b[39m\n",
      "transpose(input, dim0, dim1) -> Tensor\n",
      "\n",
      "Returns a tensor that is a transposed version of :attr:`input`.\n",
      "The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.\n",
      "\n",
      "If :attr:`input` is a strided tensor then the resulting :attr:`out`\n",
      "tensor shares its underlying storage with the :attr:`input` tensor, so\n",
      "changing the content of one would change the content of the other.\n",
      "\n",
      "If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` then the\n",
      "resulting :attr:`out` tensor *does not* share the underlying storage\n",
      "with the :attr:`input` tensor.\n",
      "\n",
      "If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` with compressed\n",
      "layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments\n",
      ":attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must\n",
      "both be sparse dimensions. The batch dimensions of a sparse tensor are the\n",
      "dimensions preceding the sparse dimensions.\n",
      "\n",
      ".. note::\n",
      "    Transpositions which interchange the sparse dimensions of a `SparseCSR`\n",
      "    or `SparseCSC` layout tensor will result in the layout changing between\n",
      "    the two options. Transposition of the sparse dimensions of a ` SparseBSR`\n",
      "    or `SparseBSC` layout tensor will likewise generate a result with the\n",
      "    opposite layout.\n",
      "\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    dim0 (int): the first dimension to be transposed\n",
      "    dim1 (int): the second dimension to be transposed\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(2, 3)\n",
      "    >>> x\n",
      "    tensor([[ 1.0028, -0.9893,  0.5809],\n",
      "            [-0.1669,  0.7299,  0.4942]])\n",
      "    >>> torch.transpose(x, 0, 1)\n",
      "    tensor([[ 1.0028, -0.1669],\n",
      "            [-0.9893,  0.7299],\n",
      "            [ 0.5809,  0.4942]])\n",
      "\n",
      "See also :func:`torch.t`.\n",
      "\u001b[31mType:\u001b[39m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = a.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is interesting to note that both tensors are lists within lists, with the inner most list being the final dimension (here the data for some row, i.e. the column data). So, if we make 3 dimensional tensor we will have the final dimension be the length of the inner most lists.\n",
    "\n",
    "Also, btw lists point to python objects all over memory. Numpy arrays ensure there is a block of memory stored for the entire 'list', and that the list is composed of a single data type that can be operated on efficiently and with special functions not applicable to lists. The same idea persists to pytorch, except with the addition of being able to push that all onto GPUs (and of course the deep learning library that comprises the other pillar of PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5080, -1.3213,  1.5132],\n",
       "        [ 0.9948,  0.3890,  0.9774],\n",
       "        [-0.8030, -0.9537, -0.1519]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_tensor = torch.randn(3,3)\n",
    "quick_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6054,  1.1398,  1.3036],\n",
       "        [ 0.4394, -1.0014, -2.0658],\n",
       "        [ 0.4413,  0.2615, -0.5047]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_tensor.inverse() # no inverse method in numpy that I am aware of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/f9tvwl412dq2pq6wwlstvskh0000gp/T/ipykernel_80005/1227932761.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  quick_tensor.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " -1.508038878440857\n",
       " -1.3213382959365845\n",
       " 1.513158917427063\n",
       " 0.994814395904541\n",
       " 0.38895902037620544\n",
       " 0.9774173498153687\n",
       " -0.8030130863189697\n",
       " -0.9536927938461304\n",
       " -0.1519087851047516\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 9]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_tensor.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 107\n",
       " 7\n",
       " 193\n",
       " 191\n",
       " 157\n",
       " 33\n",
       " 169\n",
       " 191\n",
       " 49\n",
       " 175\n",
       " 193\n",
       " 63\n",
       " 40\n",
       " 172\n",
       " 126\n",
       " 63\n",
       " 163\n",
       " 37\n",
       " 199\n",
       " 62\n",
       " 6\n",
       " 56\n",
       " 122\n",
       " 63\n",
       " 68\n",
       " 146\n",
       " 77\n",
       " 191\n",
       " 54\n",
       " 37\n",
       " 116\n",
       " 191\n",
       " 250\n",
       " 141\n",
       " 27\n",
       " 190\n",
       "[torch.storage.UntypedStorage(device=cpu) of size 36]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_tensor.untyped_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trailing underscore for methods indicate the method is performed inplace\n",
    "a = torch.ones(3,2)\n",
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nI am skipping a lot about storage. It is interesting to note\\nthat many methods applied to the tensor are just re-indexing\\nsome meta data about how the tensor is stored. \\n\\nOne main reason for the explanation of storage is that some methods\\nwill not work on non-contiguous tensors, which are typically tensors\\nderived from other contiguous tenors; e.g. the transport of A is \\nstored in the same location as A, just the meta-data updates. The\\nview method does not work with A^T. This methods is encounted in the \\nnext chapter.\\n\\nSince I don't have CUDA cores, I am also skipping the to(device='cude')\\nmethods. Another book I have has some code for pushing tensors and models\\nto Apple's M series chips' GPU. I will use that if/when necessary. \\n\\nOtherwise, this code may be nice to revisit if I get into cloud computing.\\n\\nThis discussion also leads into the idea of generalize tensors, which are \\nreally just subsets of tensors. For example, sparse tensors are stored in\\nthe same way but with extra info regarding indexing so that we know where\\nthe non-zero values actually are in the tensor. Much later (chapter 15),\\nwe will meet quantized tensors. \\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "I am skipping a lot about storage. It is interesting to note\n",
    "that many methods applied to the tensor are just re-indexing\n",
    "some meta data about how the tensor is stored. \n",
    "\n",
    "One main reason for the explanation of storage is that some methods\n",
    "will not work on non-contiguous tensors, which are typically tensors\n",
    "derived from other contiguous tenors; e.g. the transport of A is \n",
    "stored in the same location as A, just the meta-data updates. The\n",
    "view method does not work with A^T. This methods is encounted in the \n",
    "next chapter.\n",
    "\n",
    "Since I don't have CUDA cores, I am also skipping the to(device='cude')\n",
    "methods. Another book I have has some code for pushing tensors and models\n",
    "to Apple's M series chips' GPU. I will use that if/when necessary. \n",
    "\n",
    "Otherwise, this code may be nice to revisit if I get into cloud computing.\n",
    "\n",
    "This discussion also leads into the idea of generalize tensors, which are \n",
    "really just subsets of tensors. For example, sparse tensors are stored in\n",
    "the same way but with extra info regarding indexing so that we know where\n",
    "the non-zero values actually are in the tensor. Much later (chapter 15),\n",
    "we will meet quantized tensors. \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(list(range(9)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(3,3) # should we not just do reshape?\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 9]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 9]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage() == b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  0.5403, -0.4161, -0.9900, -0.6536,  0.2837,  0.9602,  0.7539,\n",
       "        -0.1455])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcos_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "a.cos_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m a.short()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcos_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "a.short()\n",
    "a.cos_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is happening here is that list(range(9)) created a list\n",
    "# with values of dtype float64, i.e. long. PyTorch is expecting\n",
    "# default values of dtype float32.\n",
    "aa = a.float()\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  0.5403, -0.4161, -0.9900, -0.6536,  0.2837,  0.9602,  0.7539,\n",
       "        -0.1455])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.cos_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning with Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
